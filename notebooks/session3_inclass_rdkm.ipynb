{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to ```spaCy```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a number of different NLP frameworks that you're likely to encounter. The most popular and widely-used of these are:\n",
    "\n",
    "- ```NLTK``` (Natural Language Toolkit, old-school)\n",
    "- ```UDPipe``` (Neural network based, fast and light, but not super accurate)\n",
    "- ```CoreNLP``` and ```stanza``` (Created by the team at Stanford; academically robust)\n",
    "- ```spaCy``` production-ready, well-documented, state-of-the-art\n",
    "\n",
    "We'll be working with ```spaCy``` in this module, primarily because it's easy and intuitive, and also scales well.\n",
    "\n",
    "First thing we need to do is install ```spaCy``` and the language model that we want to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "$ source ./lang101/bin/activate\n",
    "$ pip install pandas\n",
    "$ pip install spacy \n",
    "$ python -m spacy download en_core_web_sm\n",
    "$ deactivate \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing ```spaCy```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we need to do is import ```spaCy``` __and__ the language model that we want to use.\n",
    "\n",
    "Note that, if you want to use different langauges you want to use different language models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a spacy NLP object\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the model now loaded, we can begin to do some very simple NLP tasks.\n",
    "\n",
    "Here, we create a spaCy object and assign it to the variable ```nlp```. This is the NLP pipeline that will do all our heavy lifting, using the trained model we've specified.\n",
    "\n",
    "Below, you can see what the pipeline does with a bit of sample text. Passing text to the nlp object gives us access to a bunch of properties, including tokens (words), parts of speech, named entities, and so on. Here's we two of them, tokens and entities. These objects, in turn, have certain methods attached to them. A full outline of available methods can be found in the spaCy docs.\n",
    "\n",
    "In this case, for all token objects, let's return the token itself (token.text); its part-of-speech tag (token.pos_); and the grammatical dependency relations between the tokens (token.dep_).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"This is a test sentence written in English. You're a good worker.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Tokenize__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Trying some more attributes__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print(token.i, token.text, token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count distribution of linguistic features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Create doc object__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"example.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create counter\n",
    "adjective_count = 0\n",
    "for token in doc:\n",
    "    if token.pos_ == \"ADJ\":\n",
    "        adjective_count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Relative frequency__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_freq = adjective_count/len(doc)*1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"This text has a relative frequency of {int(relative_freq)} adjectives per 1000 tokens\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
